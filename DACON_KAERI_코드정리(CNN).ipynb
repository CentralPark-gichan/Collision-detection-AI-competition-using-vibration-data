{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DACON] 진동데이터 활용 충돌체 탐지 AI 경진대회\n",
    "## 두부(팀명) (Team name)\n",
    "## 2020년 7월 17일 (제출날짜) (Submission date)\n",
    "\n",
    "중간 중간에 코드가 미숙한 부분들이 많이 있습니다. 양해해주시면 감사하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "random.seed(777)\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgbm\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\ParkGiChan\\\\Desktop\\\\DataAnalysis\\\\DACON_KAERI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import jovian\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, Callback, EarlyStopping\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras_version : 2.1.5\n",
      "numpy_version : 1.16.6\n",
      "pandas_version : 0.25.3\n",
      "tensorflow_version :1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"keras_version : \" + str(keras.__version__))\n",
    "print(\"numpy_version : \" + str(np.__version__))\n",
    "print(\"pandas_version : \" + str(pd.__version__))\n",
    "print(\"tensorflow_version :\" + str(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Case1, Case2 (변수추가 + Time 200까지만) \n",
    "Case1 : Cosine scheduler 사용 \\\n",
    "Case2 : X \\\n",
    "-> 데이터가 적고, metric이 너무 예민해서 그런지, scheduler 혹은 lr에  따라 학습 되는 것이 너무 달라 앙상블 해줬습니다.\\\n",
    "-> LB : 0.0035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_features.csv\", index_col=0)\n",
    "test = pd.read_csv(\"test_features.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"train_target.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"sample_submission.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train['Time'] <= 0.000004*200]\n",
    "test = test.loc[test['Time'] <= 0.000004*200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform1(train, n1, n2):\n",
    "    train_time = train.copy()\n",
    "    train_time['S1'] = [1 if i != 0 else 0 for i in train['S1']]\n",
    "    train_time['S2'] = [1 if i != 0 else 0 for i in train['S2']]\n",
    "    train_time['S3'] = [1 if i != 0 else 0 for i in train['S3']]\n",
    "    train_time['S4'] = [1 if i != 0 else 0 for i in train['S4']]\n",
    "    train_time.drop(['Time'], axis=1, inplace=True)\n",
    "    \n",
    "    train_time = train_time.sum(axis=1)\n",
    "    train_time = train_time.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "    train_time = np.array(train_time).reshape(n1, n2, 1, 1)\n",
    "    \n",
    "    return train_time\n",
    "\n",
    "train_time = transform1(train, 2800, 200)\n",
    "test_time = transform1(test, 700, 200)\n",
    "\n",
    "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "\n",
    "###\n",
    "\n",
    "X_data = train.iloc[:,:]\n",
    "X_data = np.array(X_data).reshape((2800,200,5,1))\n",
    "\n",
    "X_data_test = test.iloc[:,:]\n",
    "X_data_test = np.array(X_data_test).reshape((700,200,5,1))\n",
    "\n",
    "Y_data = np.array(y_train.copy())\n",
    "\n",
    "### Merge\n",
    "\n",
    "X_data = np.concatenate([X_data, train_time], axis=2)\n",
    "X_data_test = np.concatenate([X_data_test, test_time], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_Case12 = X_data.copy()\n",
    "X_data_test_Case12 = X_data_test.copy()\n",
    "Y_data_Case12 = Y_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Case3 (kernel regularizer 사용)\n",
    "앙상블을 하면 어떨까 해서 만들어 봤습니다. \\\n",
    "변수만추가하고, kerenl regularizer을 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_features.csv\", index_col=0)\n",
    "test = pd.read_csv(\"test_features.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"train_target.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"sample_submission.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = transform1(train, 2800, 375)\n",
    "test_time = transform1(test, 700, 375)\n",
    "\n",
    "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "\n",
    "###\n",
    "\n",
    "X_data = train.iloc[:,:]\n",
    "X_data = np.array(X_data).reshape((2800,375,5,1))\n",
    "\n",
    "X_data_test = test.iloc[:,:]\n",
    "X_data_test = np.array(X_data_test).reshape((700,375,5,1))\n",
    "\n",
    "Y_data = np.array(y_train.copy())\n",
    "\n",
    "### Merge\n",
    "\n",
    "X_data = np.concatenate([X_data, train_time], axis=2)\n",
    "X_data_test = np.concatenate([X_data_test, test_time], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_Case3 = X_data.copy()\n",
    "X_data_test_Case3 = X_data_test.copy()\n",
    "Y_data_Case3 = Y_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Case4\n",
    "baseline model입니다. 유용균박사님 코드에서 cosine scheduler만 추가한 것입니다.\\\n",
    "이 또한 앙상블을 하기 위해 만들어 봤습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_features.csv\", index_col=0)\n",
    "test = pd.read_csv(\"test_features.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"train_target.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"sample_submission.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
    "\n",
    "###\n",
    "\n",
    "X_data = train.iloc[:,:]\n",
    "X_data = np.array(X_data).reshape((2800,375,5,1))\n",
    "\n",
    "X_data_test = test.iloc[:,:]\n",
    "X_data_test = np.array(X_data_test).reshape((700,375,5,1))\n",
    "\n",
    "Y_data = np.array(y_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_Case4 = X_data.copy()\n",
    "X_data_test_Case4 = X_data_test.copy()\n",
    "Y_data_Case4 = Y_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case12 nrow: 200 / Case12 ncol : 6\n",
      "Case3 nrow : 375 / Case3 ncol : 6\n",
      "Case4 nrow : 375 / Case4 ncol : 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Case12 nrow: \" + str(X_data_Case12.shape[1]) + \" / Case12 ncol : \" + str(X_data_Case12.shape[2]))\n",
    "print(\"Case3 nrow : \" + str(X_data_Case3.shape[1]) + \" / Case3 ncol : \" + str(X_data_Case3.shape[2]))\n",
    "print(\"Case4 nrow : \" + str(X_data_Case4.shape[1]) + \" / Case4 ncol : \" + str(X_data_Case4.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case12 nrow: 200 / Case12 ncol : 6\n",
      "Case3 nrow : 375 / Case3 ncol : 6\n",
      "Case4 nrow : 375 / Case4 ncol : 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Case12 nrow: \" + str(X_data_test_Case12.shape[1]) + \" / Case12 ncol : \" + str(X_data_test_Case12.shape[2]))\n",
    "print(\"Case3 nrow : \" + str(X_data_test_Case3.shape[1]) + \" / Case3 ncol : \" + str(X_data_test_Case3.shape[2]))\n",
    "print(\"Case4 nrow : \" + str(X_data_test_Case4.shape[1]) + \" / Case4 ncol : \" + str(X_data_test_Case4.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 탐색적 자료분석\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변수 선택 및 모델 구축\n",
    "기본적으로 유용균 박사님 코드에서 5fold + cosine scheduler 등등만 추가한 것입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(777)\n",
    "rn.seed(777)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(777)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import Callback\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        #if self.verbose &gt; 0:\n",
    "        #    print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "        #          'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kaeri_metric(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: KAERI metric\n",
    "    '''\n",
    "    \n",
    "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
    "\n",
    "\n",
    "### E1과 E2는 아래에 정의됨 ###\n",
    "\n",
    "def E1(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
    "    \n",
    "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
    "\n",
    "\n",
    "def E2(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = np.array([1,1,0,0])\n",
    "weight2 = np.array([0,0,1,1])\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
    "\n",
    "def my_loss_E2(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(type_id,pred,true):\n",
    "    print(pred.shape)\n",
    "\n",
    "    if type_id == 0:\n",
    "        _name = 'x_pos'\n",
    "    elif type_id == 1:\n",
    "        _name = 'y_pos'\n",
    "    elif type_id == 2:\n",
    "        _name = 'mass'\n",
    "    elif type_id == 3:\n",
    "        _name = 'velocity'\n",
    "    elif type_id == 4:\n",
    "        _name = \"distance\"\n",
    "    else:\n",
    "        _name = 'error'\n",
    "\n",
    "    x_coord = np.arange(1,pred.shape[0]+1,1)\n",
    "    if type_id < 2:\n",
    "        Err_m = (pred[:,type_id] - true[:,type_id])\n",
    "    elif type_id < 4:\n",
    "        Err_m = ((pred[:,type_id] - true[:,type_id])/true[:,type_id])*100\n",
    "    else:\n",
    "        Err_m = ((pred[:,0]-true[:,0])**2+(pred[:,1]-true[:,1])**2)**0.5\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    # plt.rcParams[\"font.family\"]=\"Times New Roman\"\n",
    "    plt.rcParams[\"font.size\"]=15\n",
    "    plt.scatter(x_coord, Err_m, marker='o')\n",
    "    plt.title(\"%s Prediction for Training Data\" % _name, size=20)\n",
    "    plt.xlabel(\"Data ID\", labelpad=10, size=20)\n",
    "    plt.ylabel(\"Prediction Error of %s,\" % _name, labelpad=10, size=20)\n",
    "    plt.xticks(size=15)\n",
    "    plt.yticks(size=15)\n",
    "    plt.ylim(-100., 100.)\n",
    "    plt.xlim(0, pred.shape[0]+1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(np.std(Err_m))\n",
    "    print(np.max(Err_m))\n",
    "    print(np.min(Err_m))\n",
    "    return Err_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(train_target, n_sam,n_col, regularizer_True):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'same'\n",
    "    model = Sequential()\n",
    "    nf = 16\n",
    "    fs = (3,1)\n",
    "    \n",
    "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(n_sam,n_col,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation ='elu'))\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    \n",
    "    if regularizer_True:\n",
    "        model.add(Dense(4, kernel_regularizer=l1_l2(l1=0.001)))\n",
    "    else:\n",
    "        model.add(Dense(4))\n",
    "    \n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    #optimizer = keras.optimizers.RMSprop()\n",
    "    \n",
    "    global weight2\n",
    "    if train_target == 1: # only for M\n",
    "        weight2 = np.array([0,0,1,0])\n",
    "    else: # only for V\n",
    "        weight2 = np.array([0,0,0,1])\n",
    "       \n",
    "    if train_target==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "       \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(model,X,Y,X_val,Y_val, save_point, enum, train_target, scheduler_True):\n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(777)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(777)\n",
    "    \n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    best_save = ModelCheckpoint('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    cosine_scheduler = CosineAnnealingScheduler(T_max=100, eta_max=6e-3, eta_min=1e-6)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    \n",
    "    my_seed = 777\n",
    "    np.random.seed(my_seed)\n",
    "    if scheduler_True:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=10000,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      validation_data=[X_val, Y_val],\n",
    "                      verbose = 2,\n",
    "                      callbacks=[best_save, early_stop,cosine_scheduler],\n",
    "                      )\n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=10000,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      validation_data=[X_val, Y_val],\n",
    "                      verbose = 2,\n",
    "                      callbacks=[best_save, early_stop],\n",
    "                      )\n",
    "    \n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(train_target, save_point, enum):\n",
    "\n",
    "    if train_target == 0:\n",
    "        model = load_model('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    else:\n",
    "        model = load_model('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5' , custom_objects={'my_loss_E2': my_loss, })\n",
    "\n",
    "    score = model.evaluate(X_data, Y_data, verbose=0)\n",
    "    print('loss:', score)\n",
    "\n",
    "    pred = model.predict(X_data)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    print('정답(original):', Y_data[i])\n",
    "    print('예측값(original):', pred[i])\n",
    "\n",
    "    print(E1(pred, Y_data))\n",
    "    print(E2(pred, Y_data))\n",
    "    #print(E2M(pred, Y_data))\n",
    "    #print(E2V(pred, Y_data))    \n",
    "    \n",
    "    if train_target ==0:\n",
    "        plot_error(4,pred,Y_data)\n",
    "    elif train_target ==1:\n",
    "        plot_error(2,pred,Y_data)\n",
    "    elif train_target ==2:\n",
    "        plot_error(3,pred,Y_data)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "lr_d = 0.0\n",
    "patience = 30\n",
    "dr_rate = 0\n",
    "\n",
    "def run_model(X_data, X_data_test, Y_data, save_point,scheduler_True,regularizer_True):\n",
    "    submit = pd.read_csv('sample_submission.csv', index_col=0)\n",
    "    submit.iloc[:,:] = 0\n",
    "\n",
    "    lr = 3e-4\n",
    "    lr_d = 0.0\n",
    "    patience = 30\n",
    "    dr_rate = 0\n",
    "    \n",
    "    n_col = X_data.shape[2]\n",
    "    n_sam = X_data.shape[1]\n",
    "    \n",
    "    for enum, (train_index,valid_index) in enumerate(group_kfold.split(X_data,Y_data,groups)):\n",
    "        X_train = X_data[train_index]\n",
    "        Y_train = Y_data[train_index]\n",
    "    \n",
    "        X_val = X_data[valid_index]\n",
    "        Y_val = Y_data[valid_index]\n",
    "    \n",
    "    \n",
    "        for train_target in range(3):\n",
    "            model = set_model(train_target, n_sam,n_col, regularizer_True)\n",
    "        \n",
    "            train_CNN(model,X_train, Y_train, X_val, Y_val, save_point, enum, train_target,scheduler_True)    \n",
    "            best_model = load_best_model(train_target, save_point, enum)\n",
    "\n",
    "   \n",
    "            pred_data_test = best_model.predict(X_data_test)\n",
    "            pred_data_valid = best_model.predict(X_val)\n",
    "    \n",
    "            if train_target == 0: # x,y 학습\n",
    "                submit.iloc[:,0] += pred_data_test[:,0]/5\n",
    "                submit.iloc[:,1] += pred_data_test[:,1]/5\n",
    "                \n",
    "            elif train_target == 1: # m 학습\n",
    "                submit.iloc[:,2] += pred_data_test[:,2]/5\n",
    "        \n",
    "            elif train_target == 2: # v 학습\n",
    "                submit.iloc[:,3] += pred_data_test[:,3]/5    \n",
    "    \n",
    "    return submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"train_target.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "groups = y_train['X'].astype('str') + \"_\" + y_train['Y'].astype('str') + \\\n",
    "    \"_\" + y_train['M'].astype('str') + \"_\" + y_train['V'].astype('str')\n",
    "group_kfold.get_n_splits(X_data, Y_data, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "keras의 경우 seed 고정을 해도, 값이 정확히 같게 나오지 않아, 같은 모델들을 3번씩 반복했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data_Case12.copy()\n",
    "X_data_test = X_data_test_Case12.copy()\n",
    "Y_data = Y_data_Case12.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_181 (Conv2D)          (None, 200, 6, 16)        64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 200, 6, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_181 (MaxPoolin (None, 100, 6, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 100, 6, 32)        1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_182 (Bat (None, 100, 6, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_182 (MaxPoolin (None, 50, 6, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_183 (Conv2D)          (None, 50, 6, 64)         6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_183 (Bat (None, 50, 6, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_183 (MaxPoolin (None, 25, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_184 (Conv2D)          (None, 25, 6, 128)        24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_184 (Bat (None, 25, 6, 128)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_184 (MaxPoolin (None, 12, 6, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_185 (Conv2D)          (None, 12, 6, 256)        98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_185 (Bat (None, 12, 6, 256)        1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_185 (MaxPoolin (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_186 (Conv2D)          (None, 6, 6, 512)         393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_186 (Bat (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_186 (MaxPoolin (None, 3, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,719,572\n",
      "Trainable params: 1,717,556\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n",
      "Train on 2240 samples, validate on 560 samples\n",
      "Epoch 1/10000\n",
      " - 14s - loss: 0.5907 - val_loss: 0.8905\n",
      "Epoch 2/10000\n",
      " - 1s - loss: 0.0307 - val_loss: 0.1848\n",
      "Epoch 3/10000\n",
      " - 1s - loss: 0.0062 - val_loss: 0.1001\n",
      "Epoch 4/10000\n",
      " - 1s - loss: 0.0031 - val_loss: 0.0214\n",
      "Epoch 5/10000\n",
      " - 1s - loss: 0.0018 - val_loss: 0.0141\n",
      "Epoch 6/10000\n",
      " - 1s - loss: 0.0013 - val_loss: 0.0234\n",
      "Epoch 7/10000\n",
      " - 1s - loss: 9.3327e-04 - val_loss: 0.0146\n",
      "Epoch 8/10000\n",
      " - 1s - loss: 8.2963e-04 - val_loss: 0.0071\n",
      "Epoch 9/10000\n",
      " - 1s - loss: 7.0034e-04 - val_loss: 0.0076\n",
      "Epoch 10/10000\n",
      " - 1s - loss: 8.8364e-04 - val_loss: 0.0032\n",
      "Epoch 11/10000\n",
      " - 1s - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 12/10000\n",
      " - 1s - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 13/10000\n",
      " - 1s - loss: 0.0013 - val_loss: 0.0040\n",
      "Epoch 14/10000\n",
      " - 1s - loss: 7.2861e-04 - val_loss: 0.0014\n",
      "Epoch 15/10000\n",
      " - 1s - loss: 4.9877e-04 - val_loss: 9.9650e-04\n",
      "Epoch 16/10000\n",
      " - 1s - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 17/10000\n",
      " - 1s - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 18/10000\n",
      " - 1s - loss: 6.5188e-04 - val_loss: 0.0045\n",
      "Epoch 19/10000\n",
      " - 1s - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 20/10000\n",
      " - 1s - loss: 8.8888e-04 - val_loss: 0.0019\n",
      "Epoch 21/10000\n",
      " - 1s - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 22/10000\n",
      " - 1s - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 23/10000\n",
      " - 1s - loss: 9.3529e-04 - val_loss: 0.0010\n",
      "Epoch 24/10000\n",
      " - 1s - loss: 7.1067e-04 - val_loss: 0.0020\n",
      "Epoch 25/10000\n",
      " - 1s - loss: 9.0359e-04 - val_loss: 7.6314e-04\n",
      "Epoch 26/10000\n",
      " - 1s - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 27/10000\n",
      " - 1s - loss: 3.4778e-04 - val_loss: 9.4790e-04\n",
      "Epoch 28/10000\n",
      " - 1s - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 29/10000\n",
      " - 1s - loss: 9.9584e-04 - val_loss: 0.0038\n",
      "Epoch 30/10000\n",
      " - 1s - loss: 5.5835e-04 - val_loss: 9.6196e-04\n",
      "Epoch 31/10000\n",
      " - 1s - loss: 6.7558e-04 - val_loss: 5.5054e-04\n",
      "Epoch 32/10000\n",
      " - 1s - loss: 4.5988e-04 - val_loss: 3.4720e-04\n",
      "Epoch 33/10000\n",
      " - 1s - loss: 8.4824e-04 - val_loss: 8.1385e-04\n",
      "Epoch 34/10000\n",
      " - 1s - loss: 3.7267e-04 - val_loss: 4.4050e-04\n",
      "Epoch 35/10000\n",
      " - 1s - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 36/10000\n",
      " - 1s - loss: 0.0013 - val_loss: 5.5859e-04\n",
      "Epoch 37/10000\n",
      " - 1s - loss: 5.8413e-04 - val_loss: 7.2997e-04\n",
      "Epoch 38/10000\n",
      " - 1s - loss: 5.4649e-04 - val_loss: 3.6171e-04\n",
      "Epoch 39/10000\n",
      " - 1s - loss: 4.2699e-04 - val_loss: 0.0010\n",
      "Epoch 40/10000\n",
      " - 1s - loss: 4.2049e-04 - val_loss: 0.0047\n",
      "Epoch 41/10000\n",
      " - 1s - loss: 6.5896e-04 - val_loss: 5.6177e-04\n",
      "Epoch 42/10000\n",
      " - 1s - loss: 5.6083e-04 - val_loss: 5.9231e-04\n",
      "Epoch 43/10000\n",
      " - 1s - loss: 2.0204e-04 - val_loss: 2.3920e-04\n",
      "Epoch 44/10000\n",
      " - 1s - loss: 4.6851e-04 - val_loss: 0.0016\n",
      "Epoch 45/10000\n",
      " - 1s - loss: 6.8628e-04 - val_loss: 3.8889e-04\n",
      "Epoch 46/10000\n",
      " - 1s - loss: 6.0791e-04 - val_loss: 4.3843e-04\n",
      "Epoch 47/10000\n",
      " - 1s - loss: 3.2066e-04 - val_loss: 0.0015\n",
      "Epoch 48/10000\n",
      " - 1s - loss: 1.8634e-04 - val_loss: 1.9577e-04\n",
      "Epoch 49/10000\n",
      " - 1s - loss: 1.4697e-04 - val_loss: 0.0012\n",
      "Epoch 50/10000\n",
      " - 1s - loss: 2.6866e-04 - val_loss: 1.8656e-04\n",
      "Epoch 51/10000\n",
      " - 1s - loss: 1.5176e-04 - val_loss: 3.7955e-04\n",
      "Epoch 52/10000\n",
      " - 1s - loss: 3.0970e-04 - val_loss: 1.8300e-04\n",
      "Epoch 53/10000\n",
      " - 1s - loss: 3.5187e-04 - val_loss: 4.7600e-04\n",
      "Epoch 54/10000\n",
      " - 1s - loss: 2.0617e-04 - val_loss: 1.8796e-04\n",
      "Epoch 55/10000\n",
      " - 1s - loss: 1.5096e-04 - val_loss: 5.9788e-04\n",
      "Epoch 56/10000\n",
      " - 1s - loss: 1.7784e-04 - val_loss: 0.0016\n",
      "Epoch 57/10000\n",
      " - 1s - loss: 2.5898e-04 - val_loss: 3.1072e-04\n",
      "Epoch 58/10000\n",
      " - 1s - loss: 1.9726e-04 - val_loss: 1.4031e-04\n",
      "Epoch 59/10000\n",
      " - 1s - loss: 1.0408e-04 - val_loss: 4.0101e-04\n",
      "Epoch 60/10000\n",
      " - 1s - loss: 1.8587e-04 - val_loss: 3.2584e-04\n",
      "Epoch 61/10000\n",
      " - 1s - loss: 1.2317e-04 - val_loss: 3.5642e-04\n",
      "Epoch 62/10000\n",
      " - 1s - loss: 1.1310e-04 - val_loss: 3.6036e-04\n",
      "Epoch 63/10000\n",
      " - 1s - loss: 8.7897e-05 - val_loss: 1.2298e-04\n",
      "Epoch 64/10000\n",
      " - 1s - loss: 6.7902e-05 - val_loss: 1.3269e-04\n",
      "Epoch 65/10000\n",
      " - 1s - loss: 7.9533e-05 - val_loss: 1.8406e-04\n",
      "Epoch 66/10000\n",
      " - 1s - loss: 1.7072e-04 - val_loss: 1.7039e-04\n",
      "Epoch 67/10000\n",
      " - 1s - loss: 1.1783e-04 - val_loss: 1.4138e-04\n",
      "Epoch 68/10000\n",
      " - 1s - loss: 9.9539e-05 - val_loss: 1.7578e-04\n",
      "Epoch 69/10000\n",
      " - 1s - loss: 1.1548e-04 - val_loss: 1.1081e-04\n",
      "Epoch 70/10000\n",
      " - 1s - loss: 7.0910e-05 - val_loss: 1.5265e-04\n",
      "Epoch 71/10000\n",
      " - 1s - loss: 8.4691e-05 - val_loss: 1.6426e-04\n",
      "Epoch 72/10000\n",
      " - 1s - loss: 1.4762e-04 - val_loss: 1.1571e-04\n",
      "Epoch 73/10000\n",
      " - 1s - loss: 8.7572e-05 - val_loss: 5.2016e-04\n",
      "Epoch 74/10000\n",
      " - 1s - loss: 1.0024e-04 - val_loss: 2.4586e-04\n",
      "Epoch 75/10000\n",
      " - 1s - loss: 9.4880e-05 - val_loss: 2.9776e-04\n",
      "Epoch 76/10000\n",
      " - 1s - loss: 8.5883e-05 - val_loss: 2.1210e-04\n",
      "Epoch 77/10000\n",
      " - 1s - loss: 1.6006e-04 - val_loss: 1.4800e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/10000\n",
      " - 1s - loss: 6.7503e-05 - val_loss: 2.3518e-04\n",
      "Epoch 79/10000\n",
      " - 1s - loss: 6.9155e-05 - val_loss: 2.3297e-04\n",
      "Epoch 80/10000\n",
      " - 1s - loss: 1.0558e-04 - val_loss: 1.1557e-04\n",
      "Epoch 81/10000\n",
      " - 1s - loss: 7.3907e-05 - val_loss: 1.7257e-04\n",
      "Epoch 82/10000\n",
      " - 1s - loss: 6.2230e-05 - val_loss: 9.8071e-05\n",
      "Epoch 83/10000\n",
      " - 1s - loss: 7.4150e-05 - val_loss: 1.2697e-04\n",
      "Epoch 84/10000\n",
      " - 1s - loss: 5.2477e-05 - val_loss: 1.2469e-04\n",
      "Epoch 85/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-cebaaa044824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmit_case1_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_data_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msubmit_case1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_data_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msubmit_case1_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_data_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-135-c3953f2050bf>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(X_data, X_data_test, Y_data, save_point, scheduler_True, regularizer_True)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_sam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer_True\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mtrain_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler_True\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_best_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-26513837703f>\u001b[0m in \u001b[0;36mtrain_CNN\u001b[1;34m(model, X, Y, X_val, Y_val, save_point, enum, train_target, scheduler_True)\u001b[0m\n\u001b[0;32m     25\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                       \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                       \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcosine_scheduler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                       )\n\u001b[0;32m     29\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "submit_case1_1 = run_model(X_data,X_data_test, Y_data, 1,True,False)\n",
    "submit_case1_2 = run_model(X_data,X_data_test, Y_data, 2,True,False)\n",
    "submit_case1_3 = run_model(X_data,X_data_test, Y_data, 3,True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data_Case12.copy()\n",
    "X_data_test = X_data_test_Case12.copy()\n",
    "Y_data = Y_data_Case12.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit_case2_1 = run_model(X_data,X_data_test, Y_data, 4,False,False)\n",
    "submit_case2_2 = run_model(X_data,X_data_test, Y_data, 5,False,False)\n",
    "submit_case2_3 = run_model(X_data,X_data_test, Y_data, 6,False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data_Case3.copy()\n",
    "X_data_test = X_data_test_Case3.copy()\n",
    "Y_data = Y_data_Case3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit_case3_1 = run_model(X_data,X_data_test, Y_data, 7,False,True)\n",
    "submit_case3_2 = run_model(X_data,X_data_test, Y_data, 8,False,True)\n",
    "submit_case3_3 = run_model(X_data,X_data_test, Y_data, 9,False,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data_Case4.copy()\n",
    "X_data_test = X_data_test_Case4.copy()\n",
    "Y_data = Y_data_Case4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit_case4_1 = run_model(X_data,X_data_test, Y_data, 10,True,False)\n",
    "submit_case4_2 = run_model(X_data,X_data_test, Y_data, 11,True,False)\n",
    "submit_case4_3 = run_model(X_data,X_data_test, Y_data, 12,True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case5 = pd.read_csv(\"Case5.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case1 = (submit_case1_1 + submit_case1_2 + submit_case1_3)/3\n",
    "Case2 = (submit_case2_1 + submit_case2_2 + submit_case2_3)/3\n",
    "Case3 = (submit_case3_1 + submit_case3_2 + submit_case3_3)/3\n",
    "Case4 = (submit_case4_1 + submit_case4_2 + submit_case4_3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = ((Case1 + Case2)/2)*0.5 + ((Case3 + Case4 + Case5)/3)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['X'] = np.clip(submission['X'], -400, 400)\n",
    "submission['Y'] = np.clip(submission['Y'], -400, 400)\n",
    "submission['M'] = np.clip(submission['M'],  25,  175)\n",
    "submission['V'] = np.clip(submission['V'],  0.2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_final.csv\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
